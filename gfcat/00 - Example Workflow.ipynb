{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a workflow for examining a specific source in the GFCat 120-second catalog, following up for more detailed analysis with gPhoton, and then generating images and movies of the target as a quality check. A lot of useful functions are defined along the way. The point of this notebook is not necessarily to recommend this specific workflow (although it is a good one), but provide a framework upon which to hang a lot of useful information and functions about how to work with the GFCat / gPhoton / GALEX data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not intended as complete documentation, however. As a start, also read:\n",
    "* Morrissey, Patrick, et al. \"The calibration and data products of GALEX.\" The Astrophysical Journal Supplement Series 173.2 (2007): 682.\n",
    "* Million, Chase, et al. \"gPhoton: The GALEX Photon Data Archive.\" The Astrophysical Journal 833.2 (2016): 292.\n",
    "* http://www.galex.caltech.edu/researcher/techdocs.html\n",
    "* Bianchi, Luciana, et al. \"New UV-source catalogs, UV spectral database, UV variables and science tools from the GALEX surveys.\" Astrophysics and space science 363.3 (2018): 56.\n",
    "* de la Vega, Alexander, and Luciana Bianchi. \"Searching for Short-timescale Variability in the Ultraviolet with the GALEX gPhoton Archive. I. Artifacts and Spurious Periodicities.\" The Astrophysical Journal Supplement Series 238.2 (2018): 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and housekeeping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML # requires ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sql\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import photutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gPhoton.MCUtils import angularSeparation, print_inline\n",
    "from gPhoton.galextools import counts2mag, mag2counts, aper2deg\n",
    "from gPhoton import PhotonPipe, gAperture\n",
    "import gfcat_utils as gfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photdir = '../photom' # Relative path to the local disk location of the photometry data\n",
    "data_directory='../data' # Relative path to the local disk location that data should be written\n",
    "band = 'NUV' # NUV has ~10x high countrates than FUV for most types of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GJ65 (aka UV Ceti) as the test source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skypos = (24.76279, -17.94948) # position of UV Ceti\n",
    "match_radius = 0.005\n",
    "eclipse = 13656 # This is one of the UV Ceti eclipses w/ a flare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking a test visit (e26980).\n",
    "\n",
    "This visit includes GJ65 (aka UV Ceti) during a time that it flares.\n",
    "\n",
    "(See _Fleming, S. W., et al. \"New Detections Of Time-Resolved GALEX Flares From The GJ 65 System.\" AAS (2020): 273-10._ w/ paper in review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photfile = f'{photdir}/e{eclipse}/e{eclipse}-nd-photom.csv' # This is one of the UV Ceti eclipses\n",
    "fn = photfile.split('/')[-1]\n",
    "exptfile = f'{photdir}/e{eclipse}/e{eclipse}-nd-exptime.csv'\n",
    "phot = pd.read_csv(photfile)\n",
    "expt = pd.read_csv(exptfile)\n",
    "total_expt = expt.expt.sum()\n",
    "print(f'Eclipse {eclipse} summary:')\n",
    "print(f'   * {round(total_expt)} seconds of exposure')\n",
    "print(f'   * {len(phot)} unique sources detected')\n",
    "print(f'   * {len(phot.loc[phot.aperture_sum_mask==0])} unflagged sources')\n",
    "print(f'\\nPhotometry file columns:\\n\\t{phot.keys()}')\n",
    "print(f'\\nExposure time file columns:\\n\\t{expt.keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GFCat photometry data are structured as follows:\n",
    "1. The photometric data are contained in a \\*-photom.csv file on a per-eclipse basis.\n",
    "    * The _id_ column is the source id for this visit / eclipse. The \"id\" value is unique per visit, but reused across visits, so that \"id + eclipse\" is unique.\n",
    "    * The _aperture\\_sum_ column is the (interpolated) sum of counts in the photometric aperture on the **full-depth** image.\n",
    "    * The _aperture\\_sum\\_\\[n\\]_ columns are the (interpolated_ sum of counts in the photometric aperture on the **movie** frames.\n",
    "    * Source detection / measurement was done with DAOphot (via `photutils`). The circular aperture used was equivalent to GALEX \"APER6,\" which is a radius of 12.8\". For more accurate absolute photometry, an aperture correction of 0.09 AB Mag is recommended in both bands.\n",
    "2. The exposure time data are contained in an \\*-exptime.csv file on a per-eclipse basis.\n",
    "    * The row indices of the file corresponded to the suffixes of _aperture\\_sum_ in the photometry file.\n",
    "        * i.e. `expt.iloc[2].expt` (in Pandas syntax) is the effective exposure time for _aperture\\_sum\\_2_.\n",
    "    * The _t0_ and _t1_ columns are the bin start and end times in units of \"GALEX time\" (defined as `UNIX_TIME - 315964800`).\n",
    "    * The _expt_ column contains the effective exposure time of GALEX within the bin, corrected for \"dead time\" effects and data dropouts.\n",
    "    \n",
    "Keep reading. You don't really need to understand this right now. Several helper functions are defined below in order to sort it all out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate that the flagging is meaningful\n",
    "Eliminating sources with flags set is a very good way to conservatively eliminate false (i.e. non-astrophysical) variables as a first pass. However, false variables may creep through. There may also be flags on perfectly good quality data. The GFCat implements two types of automated artifact flagging: \"edge\" and \"hotspot.\" The edge flag indicates that a source measurement fell near the edge of the detector, where data quality is known to be significantly degraded by a variety of effects. The hotspot flag indicates that a source measurement overlapped with the mission's hotspot mask at some point. Importantly, GFCat does not _apply_ the hotspot mask, so investigators should independently assess whether a hotspot was active during a specific measurement. One of the best ways to do this is to generate a QA image, a procedure for which is described later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.title(f'Unflagged Sources (e{eclipse})')\n",
    "for index in phot.index.values:\n",
    "    if ((phot.loc[index].aperture_sum_mask!=0) |\n",
    "        (phot.loc[index].aperture_sum_edge!=0)):\n",
    "        continue\n",
    "    cnt = phot.loc[index][\n",
    "            [\"aperture_sum_{i}\".format(i=i) for i in np.arange(len(expt))]\n",
    "        ].values\n",
    "    cps = cnt / expt.expt.values.flatten()\n",
    "    cps_err = np.sqrt(cnt) / expt.expt.values.flatten()\n",
    "    plt.errorbar(np.arange(len(expt)),cps,yerr=cps_err*3,fmt='k-',alpha=0.2)\n",
    "plt.semilogy()\n",
    "plt.xlabel('120s Bin#')\n",
    "plt.ylabel('log10(cps)')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([1,300])\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title(f'Flagged Sources (e{eclipse})')\n",
    "for index in phot.index.values:\n",
    "    if not ((phot.loc[index].aperture_sum_mask!=0) |\n",
    "        (phot.loc[index].aperture_sum_edge!=0)):\n",
    "        continue\n",
    "    cnt = phot.loc[index][\n",
    "            [\"aperture_sum_{i}\".format(i=i) for i in np.arange(len(expt))]\n",
    "        ].values\n",
    "    cps = cnt / expt.expt.values.flatten()\n",
    "    cps_err = np.sqrt(cnt) / expt.expt.values.flatten()\n",
    "    plt.errorbar(np.arange(len(expt)),cps,yerr=cps_err*3,fmt='k-',alpha=0.2)\n",
    "plt.semilogy()\n",
    "plt.xlabel('120s Bin#')\n",
    "plt.ylabel('log10(cps)')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([1,300])\n",
    "\n",
    "print(f\"Visit RA range  ==> [{np.round(phot.ra.min(),2)}, {np.round(phot.ra.max(),2)}]\")\n",
    "print(f\"Visit Dec range ==> [{np.round(phot.dec.min(),2)}, {np.round(phot.dec.max(),2)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump the source positions into an SQLite database.\n",
    "This provides a ~100000x speed up over using Dask! This might take >30 minutes, but you only ever have to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def populate_sqlite(regen=False,catdbfile = 'catalog.db',photdir = '../photom'):\n",
    "    if os.path.exists(catdbfile) and not regen:\n",
    "        print(f'{catdbfile} already exists.')\n",
    "        return\n",
    "    if os.path.exists(catdbfile) and regen:\n",
    "        os.remove(catdbfile)\n",
    "    engine = sql.create_engine(f'sqlite:///{catdbfile}', echo=False)\n",
    "    n_sources = 0\n",
    "    for i,edir in enumerate(os.listdir(photdir)):\n",
    "        if 'DS_Store' in edir: # Skip the annoying OSX cruft\n",
    "            continue\n",
    "        for csvfile in os.listdir(f'{photdir}/{edir}/'):\n",
    "            if 'nd-photom' in csvfile:\n",
    "                photpath = f'{photdir}/{edir}/{csvfile}'\n",
    "                expt = pd.read_csv(photpath.replace('-photom','-exptime'))\n",
    "                total_exptime = expt.expt.sum()\n",
    "                eclipse = int(csvfile.split('-')[0][1:])\n",
    "                photpath = f'{photdir}/{edir}/{csvfile}'\n",
    "                phot = pd.read_csv(photpath)\n",
    "                #print(total_exptime,phot.aperture_sum.values)\n",
    "                n_sources += len(phot)\n",
    "                phot['eclipse'] = eclipse\n",
    "                pos = phot[['eclipse', 'id', 'ra', 'dec', 'xcenter', 'ycenter']]\n",
    "                pos['exptime'] = total_exptime\n",
    "                pos['cps'] = np.array(phot.aperture_sum.values)/total_exptime\n",
    "                pos['cps_err'] = np.sqrt(phot.aperture_sum.values)/total_exptime\n",
    "                pos['hasmask'] = phot.aperture_sum_mask.values!=0\n",
    "                pos['hasedge'] = phot.aperture_sum_edge.values!=0\n",
    "                #pos.to_csv(photpath.replace('-photom','-pos'),index=False)\n",
    "                pos.to_sql('gfcat', con=engine, if_exists='append' if i!=0 else 'replace')\n",
    "        if not i%100:\n",
    "            print_inline(f'Ingesting: {i}')\n",
    "    # Index on RA, Dec --- this is what we're most interested in searching against\n",
    "    engine.execute(\"CREATE INDEX 'ix_gfcat' ON 'gfcat' ('ra', 'dec')\")\n",
    "    engine.dispose()\n",
    "    print_inline('Source position data dumped to SQLite.\\n')\n",
    "    return\n",
    "\n",
    "%time populate_sqlite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the database is complete.\n",
    "def gfcat_count(catdbfile='catalog.db'):\n",
    "    engine = sql.create_engine(f'sqlite:///{catdbfile}', echo=False)\n",
    "    out = engine.execute(f\"SELECT COUNT(eclipse) FROM gfcat \").fetchall()\n",
    "    engine.dispose()\n",
    "    return out[0][0]\n",
    "\n",
    "%time assert(gfcat_count()==18340721) # This is how many sources should be in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a cone search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query,catdbfile='catalog.db'):\n",
    "    # This will just run any SQL query that you feed it. The table is named \"gfcat\"\n",
    "    engine = sql.create_engine(f'sqlite:///{catdbfile}', echo=False)\n",
    "    out = engine.execute(query).fetchall()\n",
    "    engine.dispose()\n",
    "    return out\n",
    "\n",
    "%time brightstars = query(f\"SELECT eclipse, id FROM gfcat WHERE hasmask=0 AND hasedge=0 AND cps>622\") # 2x the 10% rolloff\n",
    "print(f'There are {len(brightstars)} very bright stars.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conesearch(skypos,match_radius=0.005,catdbfile='catalog.db'):\n",
    "    # This runs a box search in SQLite and then refines it into a cone\n",
    "    out = np.array(query(f\"SELECT eclipse, id, ra, dec, xcenter, ycenter FROM gfcat WHERE ra >= {skypos[0]-match_radius} AND ra <={skypos[0]+match_radius} AND dec>= {skypos[1]-match_radius} AND dec<={skypos[1]+match_radius}\"))\n",
    "    dist_ix = np.where(angularSeparation(skypos[0],skypos[1],\n",
    "                                         out[:,2],out[:,3])<=match_radius)\n",
    "    return pd.DataFrame({'eclipse':np.array(out[:,0][dist_ix],dtype='int16'),\n",
    "                         'id':np.array(out[:,1][dist_ix],dtype='int16'),\n",
    "                         'ra':out[:,2][dist_ix],\n",
    "                         'dec':out[:,3][dist_ix],\n",
    "                         'xcenter':out[:,4][dist_ix],\n",
    "                         'ycenter':out[:,5][dist_ix]})\n",
    "\n",
    "%time uvceti = conesearch((24.76279, -17.94948)) # Faster because it's searching the indexed columns\n",
    "print(f'There are {len(uvceti)} UV Ceti observations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to get and plot lightcurves given a source position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightcurves(skypos,match_radius=0.005,debug=False,catdbfile='catalog.db'):\n",
    "    engine = sql.create_engine(f'sqlite:///{catdbfile}', echo=False)\n",
    "    out = np.array(engine.execute(f\"SELECT eclipse, id, ra, dec FROM gfcat WHERE ra >= {skypos[0]-match_radius} AND ra <={skypos[0]+match_radius} AND dec>= {skypos[1]-match_radius} AND dec<={skypos[1]+match_radius}\").fetchall())\n",
    "    engine.dispose()\n",
    "    pos = pd.DataFrame({'eclipse':out[:,0],'id':out[:,1],\n",
    "                        'ra':out[:,2],'dec':out[:,3]})\n",
    "    pos['dist'] = angularSeparation(skypos[0],skypos[1],pos.ra.values,pos.dec.values)\n",
    "    if debug:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title(f'{len(pos)} sources found')\n",
    "        plt.plot(pos.ra,pos.dec,'k.',alpha=0.7)\n",
    "        plt.plot(skypos[0],skypos[1],'rx')\n",
    "        plt.xlim([skypos[0]-max(pos.dist.values)*1.1,skypos[0]+max(pos.dist.values)*1.1])\n",
    "        plt.ylim([skypos[1]-max(pos.dist.values)*1.1,skypos[1]+max(pos.dist.values)*1.1])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    data = {}\n",
    "    total_expt = 0\n",
    "    for j in np.arange(len(pos)):\n",
    "        eclipse = int(pos.iloc[j].eclipse)\n",
    "        photfile = f'{photdir}/e{eclipse}/e{eclipse}-nd-photom.csv'\n",
    "        exptfile = f'{photdir}/e{eclipse}/e{eclipse}-nd-exptime.csv'\n",
    "        phot = pd.read_csv(photfile)\n",
    "        expt = pd.read_csv(exptfile)\n",
    "        total_expt += expt.expt.sum()\n",
    "        cnt = phot.iloc[j][\n",
    "                [\"aperture_sum_{i}\".format(i=i) for i in np.arange(len(expt))]\n",
    "            ].values\n",
    "        cps = cnt / expt.expt.values.flatten()\n",
    "        cps_err = np.sqrt(cnt) / expt.expt.values.flatten()\n",
    "        if debug:\n",
    "            plt.figure(figsize=(15,3))\n",
    "            plt.title(f'{j} : e{eclipse} : {int(pos.iloc[j].id)} : {pos.iloc[j].dist}')\n",
    "            plt.errorbar(np.arange(len(expt)),cps,yerr=cps_err*3,fmt='k-',alpha=0.2)\n",
    "            plt.ylabel('cps')\n",
    "            plt.xticks([])\n",
    "        data[j] = {'eclipse':eclipse,\n",
    "                   'id':pos.iloc[j].id,\n",
    "                   't0':expt.t0.min(),\n",
    "                   't1':expt.t1.max(),\n",
    "                   'photfile':photfile, # filepath\n",
    "                   'exptfile':exptfile, # filepath\n",
    "                   'photdata':phot.loc[phot.id==pos.iloc[j].id], # only photometry for source of interest\n",
    "                   'exptdata':expt,\n",
    "                   'counts':cnt,\n",
    "                   'cps':cps,\n",
    "                   'cps_err':cps_err}\n",
    "    print(f'Returning {int(total_expt)} seconds of exposure.')\n",
    "    return data\n",
    "\n",
    "%time data = get_lightcurves((24.76279, -17.94948))\n",
    "print(f'\\nEach visit has an index (n={len(data.keys())}):\\n{data.keys()}')\n",
    "print(f'\\nVisit #0 keys:\\n{data[0].keys()}')\n",
    "print(f'\\teclipse: e{(eclipse:=data[0][\"eclipse\"])}')\n",
    "print(f'\\tsource#: {(sid:=int(data[0][\"id\"]))}')\n",
    "print(f'\\nPhotometry columns:\\n{data[0][\"photdata\"].keys()}')\n",
    "print(f'\\nWe need the pixel coordinates for later: ({(x:=int(data[0][\"photdata\"].xcenter))}, {(y:=int(data[0][\"photdata\"].ycenter))})')\n",
    "print(f'\\nWe also need the time range for later: [{(t0:=data[0][\"t0\"])}, {(t1:=data[0][\"t1\"])}]')\n",
    "\n",
    "# WARNING: Photutils indexes starting at 1 whereas numpy and Pandas starts indexing\n",
    "#   at zero. The \"id\" column is defined by the photutils output. So do not try to\n",
    "#   index with \"id\" using Pandas `iloc` because it will be wrong by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to return photometry data given an eclipse / source id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightcurve(eclipse,sid,photdir = '../photom'):\n",
    "    # given an eclipse and source id, return the lightcurve data\n",
    "    phot = pd.read_csv(f'{photdir}/e{eclipse}/e{eclipse}-nd-photom.csv')\n",
    "    expt = pd.read_csv(f'{photdir}/e{eclipse}/e{eclipse}-nd-exptime.csv')\n",
    "    total_expt = expt.expt.sum()\n",
    "    cnt = np.array([phot.loc[phot.id==sid][f\"aperture_sum_{i}\"].values[0] for i in np.arange(len(expt))])\n",
    "    exptimes = expt.expt.values,\n",
    "    cps = cnt / exptimes\n",
    "    cps_err = np.sqrt(cnt) / exptimes\n",
    "    return pd.DataFrame({'t0':expt.t0,'t1':expt.t1,\n",
    "                         'expt':exptimes[0],'counts':cnt,\n",
    "                         'cps':(cnt / exptimes[0]),\n",
    "                         'cps_err':(np.sqrt(cnt) / exptimes[0])})\n",
    "\n",
    "%time lc120 = get_lightcurve(eclipse,sid,photdir=photdir)\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.title(f'e{eclipse} : {sid}')\n",
    "plt.errorbar(np.array(lc120['t0'])-min(lc120['t0']),lc120['cps'],yerr=lc120['cps_err']*3,fmt='k-',alpha=0.6)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('cps')\n",
    "plt.xticks([])\n",
    "\n",
    "# Note: There are error bars on this. They are just smaller than the pixels in this view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate gPhoton lightcurves of the target\n",
    "\n",
    "This will query a database of calibrated photon events at [MAST / STScI](https://archive.stsci.edu/prepds/gphoton/) to generate a higher resolution light curve (w/ 10s bins).\n",
    "\n",
    "The runtime bottleneck is HTTPS response, and it could take >30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsz=10 # seconds of lightcurve resolution\n",
    "gaperfile = '{d}/e{e}/e{e}-{b}d-{s}s.csv'.format(\n",
    "        d=data_directory, e=eclipse, b='n' if band=='NUV' else 'f', s=\"{:03d}\".format(stepsz)) # output filename\n",
    "if not os.path.exists(gaperfile):\n",
    "    # This can take a many minutes to run. The bottleneck is networking overhead.\n",
    "    lc010 = gAperture(band,skypos,aper2deg(6),trange=[t0,t1],stepsz=stepsz,verbose=2,csvfile=gaperfile)\n",
    "else:\n",
    "    lc010 = pd.read_csv(gaperfile)\n",
    "    print(f'{gaperfile} exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lc(ax):\n",
    "    ax.errorbar(lc010['t0']+5,counts2mag(lc010['cps'],'NUV'), # This function converts GALEX countrate to AB Mag\n",
    "             yerr=counts2mag(lc010['cps'],'NUV')-counts2mag(lc010['cps']+lc010['cps_err']*3,'NUV'),\n",
    "# The errors in AB Mag (i.e. log-space) are actually asymmetric. This is a linear approximation valid near zero.\n",
    "             fmt='r-',label='from gAperture (10s bins)')\n",
    "    ax.errorbar(np.array(lc120['t0'])+60, # offset the timestamp to the bin center\n",
    "             counts2mag(lc120['cps'],'NUV'),\n",
    "             yerr=counts2mag(lc120['cps'],'NUV')-counts2mag(lc120['cps']+lc120['cps_err']*3,'NUV'),\n",
    "             fmt='bx-',alpha=0.6,label='from images (120s bins)')\n",
    "    #ax.set_xlabel('time (s)')\n",
    "    ax.set_ylabel('AB Mag')\n",
    "    ax.set_xlim([lc010['t0'].min(),lc120['t1'].max()-60])\n",
    "    ax.set_xticks([])\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)\n",
    "plot_lc(ax1)\n",
    "ax1.set_ylim([19,12])\n",
    "ax1.set_title('Comparison of gPhoton (10s) to image-derived (120s) NUV photometry')\n",
    "ax1.legend()\n",
    "\n",
    "# Small flare\n",
    "ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=1,rowspan=2)\n",
    "plot_lc(ax2)\n",
    "ax2.set_xlim([lc120['t0'][1],lc120['t1'][3]])\n",
    "ax2.set_ylim([19,16])\n",
    "\n",
    "# Big flare\n",
    "ax3 = plt.subplot2grid((3, 3), (1, 1), colspan=2, rowspan=2)\n",
    "plot_lc(ax3)\n",
    "ax3.set_xlim([lc120['t0'][9],lc120['t1'].max()-60])\n",
    "ax3.set_ylim([19,12])\n",
    "ax3.set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gAperture and GFCat light curves had better match, because they are derived from the same data... and they do!\n",
    "\n",
    "Note that even the ~30-second long flare (lower left) creates a significant (>6-sigma) bump in photometry in the 120-second binned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QA Images & Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the raw data from MAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw6file = gfu.download_raw6(eclipse,band,data_directory=data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photonfile = '{d}/e{e}/e{e}-{b}d.h5'.format(\n",
    "        d=data_directory, e=eclipse, b='n' if band=='NUV' else 'f') # output filename\n",
    "print(f'Photon data file: {photonfile}')\n",
    "if not os.path.exists(photonfile):\n",
    "    PhotonPipe.photonpipe(raw6file[:-13],band,raw6file=raw6file,verbose=2)\n",
    "else:\n",
    "    print('\\tAlready exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcalfilename = photonfile.replace('.h5','-xcal.h5')\n",
    "print(f'E[x]tended photon data file: {xcalfilename}')\n",
    "if not os.path.exists(xcalfilename):\n",
    "    events = gfu.calibrate_photons(photonfile,band)\n",
    "    if len(events):\n",
    "        print('Writing {xcalfilename}'.format(xcalfilename=xcalfilename))\n",
    "        if os.path.exists(xcalfilename):         # This should never happen, but...\n",
    "            os.remove(xcalfilename)              # It's important that file doesn't already exist!\n",
    "        with pd.HDFStore(xcalfilename) as store: # \n",
    "            store.append('events',events)        # Or else it will append duplicate data.\n",
    "    else:\n",
    "        raise('There is no valid data in this visit and everything after will fail.')\n",
    "else:\n",
    "    print('\\tAlready exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create full-depth images and 120s movies\n",
    "This _might_ hit CPU and memory hard, and little has been done to mitigate that. 32Gb of onboard memory is recommended, but it might work with less. For unusually bright fields, 128Gb is probably needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntfilename = photonfile.replace(\".h5\", \"-cnt.fits.gz\")\n",
    "movfilename = photonfile.replace(\".h5\", \"-mov.fits.gz\")\n",
    "print(f'Image file: {cntfilename}')\n",
    "print(f'Movie file: {movfilename}')\n",
    "# This is the biggest processing bottleneck... will hit CPU and memory hard!\n",
    "if not os.path.exists(cntfilename) or not os.path.exists(movfilename):\n",
    "    %time gfu.make_images(eclipse,band,data_directory=data_directory,bins=[\"\",120])\n",
    "#Useful tip:          This number defines the movie frame duration in seconds.^^^\n",
    "#                     So setting it to 30 would produce 30-second movies, etc.^^^\n",
    "else:\n",
    "    print('\\tAlready exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The images are structured as follows:\n",
    "1. Full depth images have \"cnt\" in the filename (in reference to the mission-produced \"count\" maps).\n",
    "    * These have been corrected for detector response, but not exposure time.\n",
    "    * The calibrated (aka \"effective\") exposure time appears in the header. So divide the image by this to get the \"intensity\" map.\n",
    "        * Images are stored this way to simplify calculating counting errors. The countrate is `counts / exptime` and the 1-sigma error is `sqrt(counts) / exptime`.\n",
    "    * There are two backplane images.\n",
    "        1. One is an image of all of the events covered by the hotspot mask.\n",
    "        2. Another is an image of all of the events >400 \"flat pixels\" from the center of the detector. (in arcsecs?)\n",
    "    * The \"edge\" and \"mask\" flags are generated by running aperture photometry identically on the backplanes as on the main image. Any amount of edge of mask \"flux\" within the aperture sets the flag as True.\n",
    "2. Movie files have \"mov\" in the filename.\n",
    "    * The time ranges and effective exposure times for each frame appear in header keywords with the frame index as a suffix, e.g. `EXPTIME_1` for the exposure time of the first frame.\n",
    "    \n",
    "Count and movie files are created with identical World Coordinate System (WCS) information. The intention is that source _detection_ be performed on the full-depth images and then source _measurement_ can be performed identically on all of the movie frames.\n",
    "    \n",
    "The following convenience function---`read_image()`---will parse the useful information out of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# m x n x t primary hdu\n",
    "# then two m x n x t backplanes for hotspot and edge\n",
    "# movie mask -- flag variable sources\n",
    "\n",
    "# make_images\n",
    "# then make_photometry -- we'll want to actually mess with parameters of source\n",
    "# detection and assess completeness (type i and type ii errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntmap, flagmap, edgemap, wcs, trange, exptime = gfu.read_image(cntfilename)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('Full-frame, full-depth')\n",
    "plt.imshow(np.zeros(np.shape(cntmap)),cmap=\"Greys_r\")\n",
    "plt.imshow(1/np.sqrt(cntmap), cmap=\"Greys\", origin=\"lower\")\n",
    "plt.imshow(1/np.sqrt(edgemap), origin=\"lower\", cmap=\"cool\") # \"edge\" events are blueish\n",
    "plt.imshow(1/np.sqrt(flagmap), origin=\"lower\", cmap=\"Wistia\") # \"hotspot\" events are yellowish\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'Time range: {trange}\\n')\n",
    "print(f'Exposure time: {exptime}\\n')\n",
    "print(f'{wcs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This generates an animated movie of the scene and can take a few minutes...\n",
    "# But it's worth it. The end result is really neat.\n",
    "movmap, _, _, wcs, tranges, exptimes = gfu.read_image(movfilename)\n",
    "    \n",
    "print(f'\\n{len(movmap)} images frames.\\n')\n",
    "print(f'Time ranges: {tranges}\\n')\n",
    "print(f'Exposure times: {exptimes}\\n')\n",
    "print(f'{wcs}')\n",
    "\n",
    "# Create an animated image\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.title('Full-frame movie')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "ims = []\n",
    "for i,frame in enumerate(movmap[:-1]): # eliminate the last frame, which always has lower exposure\n",
    "    ims.append([\n",
    "        plt.imshow(np.zeros(np.shape(frame)),cmap=\"Greys_r\", animated=True),\n",
    "        plt.imshow(1/np.sqrt(frame/exptimes[i]), cmap=\"Greys\", origin=\"lower\", animated=True),\n",
    "        plt.imshow(1/np.sqrt(edgemap), origin=\"lower\", cmap=\"cool\", animated=True),\n",
    "        plt.imshow(1/np.sqrt(flagmap), origin=\"lower\", cmap=\"Wistia\", animated=True),\n",
    "    ])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat=True)\n",
    "\n",
    "HTML(ani.to_html5_video())\n",
    "#HTML(ani.to_jshtml()) # for an interactive animation\n",
    "\n",
    "#NOTE: I don't know why this is displaying a duplicate image under the movie or how to make it stop doing that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate an image stamp of the target source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 100 # half height / width of stamp in pixels\n",
    "x,y = (1475, 1647) # pixel (xcenter, ycenter) copied from previous notebook\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(np.log10(cntmap[y - sz : y + sz, x - sz : x + sz]),origin='lower',cmap='Greys_r')\n",
    "plt.imshow(np.log10(edgemap[y - sz : y + sz, x - sz : x + sz]),origin='lower',cmap='cool')\n",
    "plt.imshow(np.log10(flagmap[y - sz : y + sz, x - sz : x + sz]),origin='lower',cmap='Wistia')\n",
    "# Now overplot the aperture\n",
    "photutils.CircularAperture([sz,sz], r=8.533333333333326).plot(color='red', lw=2, alpha=1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And generate an movie stamp. Why not? It's the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.title(\"It's a flare, y'all!\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "sz = 100 # half height / width of stamp in pixels\n",
    "x,y = (1475, 1647) # pixel (xcenter, ycenter) copied from previous notebook\n",
    "\n",
    "ims = []\n",
    "for i,frame in enumerate(movmap[:-1]):\n",
    "    ims.append([\n",
    "        plt.imshow(1/np.sqrt(frame[y - sz : y + sz, x - sz : x + sz]/exptimes[i]),origin='lower',cmap='Greys', animated=True),\n",
    "        plt.imshow(1/np.sqrt(edgemap[y - sz : y + sz, x - sz : x + sz]),origin='lower',cmap='cool', animated=True),\n",
    "        plt.imshow(1/np.sqrt(flagmap[y - sz : y + sz, x - sz : x + sz]),origin='lower',cmap='Wistia', animated=True),\n",
    "        #photutils.CircularAperture([sz,sz], r=8.533333333333326).plot(color='red', lw=2, alpha=1),\n",
    "    ])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat=True)\n",
    "\n",
    "HTML(ani.to_html5_video())\n",
    "#HTML(ani.to_jshtml())\n",
    "\n",
    "#NOTE: I don't know why this is displaying an image under the movie or how to make it stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}